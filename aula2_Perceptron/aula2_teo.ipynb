{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 2 TEORIA- Multi-Layer Perceptron\n",
    "\n",
    "Aula 2- Aula Assíncrona\n",
    "\n",
    "---\n",
    "\n",
    "## Perceptrons e Multi-layer Perceptrons\n",
    "\n",
    "- Modelos lineares, implementação básica em pytorch e inicialização\n",
    "- Perceptron\n",
    "- Multi-layer perceptrons\n",
    "- Forward propagation, backward propagation\n",
    "- Entropia Cruzada\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão e Classificação\n",
    "\n",
    "### Modelo linear\n",
    "\n",
    "* Para uma feature (atributo):\n",
    "    * y = w x + b\n",
    "\n",
    "- Para mais features (2 ou mais):\n",
    "    - y = w1 x1 + w2 x2 + b\n",
    "\n",
    "* para n feautres:\n",
    "    * y = w1 x1 + ... wn xn + d\n",
    "    * ou usamos notação matricial\n",
    "\n",
    "- **notação matricial:**\n",
    "    - y = w<sup>T</sup> x + b\n",
    "\n",
    "* para cada elemento: y = X w + b\n",
    "\n",
    "*obs: o último b vai ser somado com broadcasting*\n",
    "\n",
    "* temos uma l<sup>(i)</sup>(w,b) para cada medida e uma L(w,b) para tudo\n",
    "    * usaremos normalmente a perda quadrática\n",
    "\n",
    "- buscamos os parametros que minimizam a perda média com o treinamento: \n",
    "    - w', b' = arg min L(w,b)\n",
    "\n",
    "* usaremos o MSGD (minibatch stochastic gradient descent):\n",
    "    * minibatch de tamanho B\n",
    "    * passo (agressividade) $a$\n",
    "    * iniciar parametros (aleatoriamente)\n",
    "    * usar o negativo do gradiente para **atualizar** os parametros\n",
    "\n",
    "![alt text](graddesc.jpg \"Title\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "* até agora estavámos falando de algoritmos lineares gerais, para agora vamos falar de algoritmos lineares de redes neurais (baseados em neurônios)\n",
    "    * começaremos pela mais simples e clássica Perceptron\n",
    "\n",
    "- Neurônio\n",
    "    - entrada: vetor \n",
    "    - saída: valor único\n",
    "        - peso w (conexão) e bias b (intercepto)\n",
    "    - treinar é ajustar cada w e b\n",
    "\n",
    "*obs: há uma função de ativação que é aplicada nessa soma*\n",
    "\n",
    "<img src=\"neuronio.jpg\" width=\"300\"/>\n",
    "\n",
    "* função de ativação\n",
    "    * exemplo: Sigmoid, ReLU e Leaky ReLU\n",
    "    * no geral elas limitam o score para entre 0 e 1 (facilita a comparação) e trunca entre alto e baixo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron\n",
    "\n",
    "* expandir a perceptron\n",
    "    * exemplo: classificação de digitos (0 a 9)\n",
    "        * entrada com imagens 28x28 = 784 pixels (features)\n",
    "        * perceptron\n",
    "        * SGD com 32 imagens no batch\n",
    "        * saída: 10 classes (0 a 9)\n",
    "\n",
    "### Classificador\n",
    "\n",
    "- Nova função de ativação: Softmax\n",
    "    - interessante para normalizar a saída de forma a somar 1\n",
    "    - interpretar cada valor como uma probabilidade\n",
    "    - distribuição de probabilidades das classes i\n",
    "    - <img src=\"softmax.jpg\" width=\"250\">\n",
    "\n",
    "* O rótulo com o valor real passa a ter 1 e 0 (one hot encoding)\n",
    "    * y = [0.0 0.25 0.75] (medida)\n",
    "    * y' = [0 1 0] (rótulo)\n",
    "\n",
    "### Rede Neural Rasa\n",
    "\n",
    "* uma única camada: \\\n",
    "<img src=\"NN_4_class.jpg\" width=\"350\">\n",
    "\n",
    "- exemplo do digito\n",
    "    - entrada: 784\n",
    "    - saída: 10\n",
    "    - batch: 32\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a33b474888067a6169f866e52f630d6f3672d35114c8362b477a93e2a003ce7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
