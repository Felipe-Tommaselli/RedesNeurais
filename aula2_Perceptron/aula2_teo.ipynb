{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 2 TEORIA- Multi-Layer Perceptron\n",
    "\n",
    "Aula 2- Aula Assíncrona\n",
    "\n",
    "---\n",
    "\n",
    "## Perceptrons e Multi-layer Perceptrons\n",
    "\n",
    "- Modelos lineares, implementação básica em pytorch e inicialização\n",
    "- Perceptron\n",
    "- Multi-layer perceptrons\n",
    "- Forward propagation, backward propagation\n",
    "- Entropia Cruzada\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão e Classificação\n",
    "\n",
    "### Modelo linear\n",
    "\n",
    "*obs: Feature = atrbibuto = entrada*\n",
    "\n",
    "* Para uma feature:\n",
    "    * y = w x + b\n",
    "\n",
    "- Para mais features (2 ou mais):\n",
    "    - y = w1 x1 + w2 x2 + b\n",
    "\n",
    "* para n feautres:\n",
    "    * y = w1 x1 + ... wn xn + d\n",
    "    * ou usamos notação matricial\n",
    "\n",
    "- **notação matricial:**\n",
    "    - y = w<sup>T</sup> x + b\n",
    "\n",
    "* para cada elemento: y = X w + b\n",
    "\n",
    "*obs: o último b vai ser somado com broadcasting*\n",
    "\n",
    "* temos uma l<sup>(i)</sup>(w,b) para cada medida e uma L(w,b) para tudo\n",
    "    * usaremos normalmente a perda quadrática\n",
    "\n",
    "- buscamos os parametros que minimizam a perda média com o treinamento: \n",
    "    - w', b' = arg min L(w,b)\n",
    "\n",
    "* usaremos o MSGD (minibatch stochastic gradient descent):\n",
    "    * minibatch de tamanho B\n",
    "    * passo (agressividade) $a$\n",
    "    * iniciar parametros (aleatoriamente)\n",
    "    * usar o negativo do gradiente para **atualizar** os parametros\n",
    "\n",
    "![alt text](graddesc.jpg \"Title\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "* até agora estavámos falando de algoritmos lineares gerais, para agora vamos falar de algoritmos lineares de redes neurais (baseados em neurônios)\n",
    "    * começaremos pela mais simples e clássica Perceptron\n",
    "\n",
    "- Neurônio\n",
    "    - entrada: vetor \n",
    "    - saída: valor único\n",
    "        - peso w (conexão) e bias b (intercepto)\n",
    "    - treinar é ajustar cada w e b\n",
    "\n",
    "*obs: há uma função de ativação que é aplicada nessa soma*\n",
    "\n",
    "<img src=\"neuronio.jpg\" width=\"300\"/>\n",
    "\n",
    "* função de ativação\n",
    "    * exemplo: Sigmoid, ReLU e Leaky ReLU\n",
    "    * no geral elas limitam o score para entre 0 e 1 (facilita a comparação) e trunca entre alto e baixo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron\n",
    "\n",
    "* expandir a perceptron\n",
    "    * exemplo: classificação de digitos (0 a 9)\n",
    "        * entrada com imagens 28x28 = 784 pixels (features)\n",
    "        * perceptron\n",
    "        * SGD com 32 imagens no batch\n",
    "        * saída: 10 classes (0 a 9)\n",
    "\n",
    "### Classificador\n",
    "\n",
    "- Nova função de ativação: Softmax\n",
    "    - interessante para normalizar a saída de forma a somar 1\n",
    "    - interpretar cada valor como uma probabilidade\n",
    "    - distribuição de probabilidades das classes i\n",
    "    - <img src=\"softmax.jpg\" width=\"250\">\n",
    "\n",
    "* O rótulo com o valor real passa a ter 1 e 0 (one hot encoding)\n",
    "    * y = [0.0 0.25 0.75] (medida)\n",
    "    * y' = [0 1 0] (rótulo)\n",
    "\n",
    "### Rede Neural Rasa\n",
    "\n",
    "* uma única camada: \\\n",
    "<img src=\"NN_4_class.jpg\" width=\"350\">\n",
    "\n",
    "- exemplo do digito\n",
    "    - entrada: 784 (pixels)\n",
    "    - saída: 10 (0 a 9)\n",
    "    - batch: 32\n",
    "\n",
    "* y = X W + b\n",
    "    * X tem tamanho 32x784 (linha = imagem, coluna = pixel)\n",
    "    * y tem tamanho 10\n",
    "    * logo> [ 10 ] = [32 x 784] * W + b\n",
    "        * portanto, **W tem tamanho [784 x 10] e b tem tamanho [ 10 ]**\n",
    "    \n",
    "- <img src=\"exemplo_digito.jpg\" width=\"400\">\n",
    "- y será uma matriz de 32 linhas por 10 colunas, sendo cada coluna uma probabilidade\n",
    "- somando as 10 colunas em cada linha, tmeos 1 (probabilidade 100%)\n",
    "\n",
    "*obs: lembrando que são 32 linhas, pq queremos classificar 32 imagens, ou seja, queremos uma resposta da rede para cada imagem*\n",
    "\n",
    "### Rede Neural Profunda\n",
    "\n",
    "- adição de camadas ocultas para dividr o problema em partes menores\n",
    "- <img src=\"camadas_NN.jpg\" width=\"400\">\n",
    "\n",
    "* **Feed forward:** todos os neurônios processam a entrada, e computamos a perda\n",
    "\n",
    "- **Backpropagation:** algoritmo que usa a regra da cadeia para calcular o gradiente da função de perda, e propaga esse gradiente pelas camadas e neurônios\n",
    "    - sempre da última para primeira (assim já sabemos o resultado da classificação e o erro)\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretação do Perceptron\n",
    "\n",
    "* cada neuronio da saida é uma reta em um plano de N dimensões que cada objeto a ser classificado é um ponto nesse espaço\n",
    "* <img src=\"grafico.jpg\" width=\"300\">\n",
    "* exemplo para caso binário: duas possibiliade de classes\n",
    "\n",
    "\n",
    "### Perda Entropia Cruzada\n",
    "\n",
    "* \"para codificar dados amostrados de P precisamos ao menos de H[P] bits\", sendo:\n",
    "* H[P] a entropia de uma distribuição P:\n",
    "    * <img src=\"entropia.jpg\" width=\"300\">\n",
    "\n",
    "### Log-Likelihood (verossimilhança)\n",
    "\n",
    "* softmax gera um vetor y^ com probabiliades condicionais para cada classe\n",
    "    * y^ = P(y = 'gato' | x), ou seja, y é a probabilidade da imagem y ser um gato dado uma imagem x\n",
    "\n",
    "- Podemos comparar yˆ com y verificando quão provável as classes reais são com relação à nosso modelo dadas as entradas:\n",
    "    -  P(Y|X) = Produtorio de P(yi | xi), de i até n\n",
    "    - precisamos maximizar P(Y|X), para isso vamos utilizar log pra converter para um problema de minimização (aplicar -log dos dois lados)\n",
    "\n",
    "* assim, para cada par de rótulo y e a predição do modelo y^ ao longo de c classes, o custo é:\n",
    "    * <img src=\"likelihood.jpg\" width=\"250\">\n",
    "\n",
    "- exemplo:\n",
    "    - y = [0, 1, 0] e y^ = [0, 0.25, 0.75]\n",
    "    - l(y, y^) = \n",
    "        - 0.log(0) = 0\n",
    "        - 1.log(0.25) = -1*(-0.6) = 0.6\n",
    "        - 0.log(0.75) = 0\n",
    "    - l(y,y^) = 0.6\n",
    "        - ou seja, temos uma likelihood de 0.6, o que não é muito bom no fim das contas\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a33b474888067a6169f866e52f630d6f3672d35114c8362b477a93e2a003ce7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
