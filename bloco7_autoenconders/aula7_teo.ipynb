{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 7 TEORIA- Redes Auto-enconders/auto-associativas\n",
    "\n",
    "Semana 10-14/10 - Aula assíncrona\n",
    "> https://www.youtube.com/watch?v=O6C6wl357-o&ab_channel=MoacirAntonelliPonti\n",
    "\n",
    "\n",
    "Auto-encoder e tarefa de reconstrução\n",
    "Denoising Autoencoders\n",
    "Variational Autoencoders\n",
    "\n",
    "---\n",
    "\n",
    "## Regularização, Normalização e Transferência de Aprendizado\n",
    "\n",
    "- Autoenconders\n",
    "- Undercomplete\n",
    "- Overcomplete\n",
    "- Denoising Autoencoders\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "\n",
    "- Autoencoders são redes neurais que tem como objetivo aprender uma representação compacta dos dados de entrada.\n",
    "    - redes neurais que buscam aprender SEM RÓTULOS -> **NÃO SUPERVISIONADO**\n",
    "\n",
    "* autoenconders associam a entrada e a saída (representação compacta)\n",
    "    * x -> encoder -> x^ (representação compacta)\n",
    "    * <img src=\"encoder_decoder.jpg\" width=\"400\">\n",
    "\n",
    "- **Encoder:** aprende um *códigos* que representa a entrada (também chamado de *representação latente* ou *feature embedding*)\n",
    "    - h = s(Wx + b) = f(x)\n",
    "\n",
    "* **Decoder:** aprende a reconstruir a entrada a partir do código\n",
    "    * x^ = s(W^Th + b^T) = g(h)\n",
    "\n",
    "- exemplo: digito\n",
    "    - entrada: 28x28 = 784 pixels\n",
    "    - saída: 28x28 = 784 pixels\n",
    "    - representação latente: 10 dimensões\n",
    "    - código resistringiu em 10 o espaço de representações -> menor resolução\n",
    "    - <img src=\"digito.jpg\" width=\"300\">\n",
    "\n",
    "* saída x^ = g(f(x)), com isso minimizamos o erro na recostrução da entrada com:\n",
    "    * L(x, x^) = L(x, g(f(x)))\n",
    "    * exemplo: L(x,x^) = ||x - x^||^2 (mean squadred error)\n",
    "\n",
    "- **tipos de autoenconder:**\n",
    "    - **undercomplete:** número de neurônios na camada oculta é menor que o número de neurônios na camada de entrada\n",
    "        - a camada do código é chamada de gargalo ou \"bottleneck\" por ser restrita\n",
    "    - **overcomplete:** número de neurônios na camada oculta é maior que o número de neurônios na camada de entrada\n",
    "        - há diferentes versões desse tipo para compensar a falta de restrição do código\n",
    "\n",
    "* *por que usar um autoenconder?*\n",
    "    * a fubnção h do código ela aprende as característica de x, enquanto isso ela pode diminuir a dimensionalidade dos dados (restrição)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a33b474888067a6169f866e52f630d6f3672d35114c8362b477a93e2a003ce7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
