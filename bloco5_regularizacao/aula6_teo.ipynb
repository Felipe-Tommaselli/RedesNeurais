{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 6 TEORIA- egularização, Normalização e Transferência de Aprendizado\n",
    "\n",
    "05/10 - Aula presencial\n",
    "\n",
    "---\n",
    "\n",
    "## Regularização, Normalização e Transferência de Aprendizado\n",
    "\n",
    "- Generalização e complexidade de modelos\n",
    "- Overfitting/underfitting\n",
    "- Técnicas de regularização: funções de custo regularizadas, dropout\n",
    "- Early stopping\n",
    "- Data augmentation\n",
    "- Normalização por batch e camada\n",
    "- Transferência de aprendizado\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento de redes profundas em cenários reais\n",
    "\n",
    "### Suposições para convergência e aprendizado\n",
    "\n",
    "- **suposições Dados de treinamento**:\n",
    "    - Limpos\n",
    "    - Representativos e bem definidos\n",
    "    - Baixa taxa de erro de rótulo\n",
    "    - Quantidade de dados é suficiente\n",
    "\n",
    "* E se não for possível?\n",
    "    * overfitting, baixa generalização, dificuldade de treinamento\n",
    "\n",
    "- **Complexidade de modelos**:\n",
    "    - algoritmo ajusta uma f a partir de um espço de funções F\n",
    "        - *\"muitas\" funções:* mais graus de liberdade, menor garantia de convergência, possível overfitting\n",
    "        - *\"poucas\" funções:* menos graus de liberdade, maior garantia de convergência, possível underfitting\n",
    "        - <img src=\"espaco_funcoes.png\" width=\"300\"> \n",
    "\n",
    "* ou seja:\n",
    "    * mais funções -> mais opções para conter a função ótima do problema, porém, mais dificil será achar a melhor função do conjunto escolhido.\n",
    "    * menos funções -> mais fácil achar a melhor função do conjunto, porém, talvez ela não seja ótima o suficiente\n",
    "\n",
    "- obs: complexidade de modelos = \"viés\" segundo a Teoria do Aprendizado Estatístico\n",
    "\n",
    "* exemplo com KNN (K nearst neighboughrsj):\n",
    "    * ver slide (ta sem explicacao mas era sobre overfitting e underfitting)\n",
    "\n",
    "- **viés x variância**:\n",
    "    - complexidade baixa: pouca variancia e muito viés\n",
    "        - underfitting\n",
    "    - complexidade alta: muita variancia e pouco viés\n",
    "        - overfitting\n",
    "    - complexidade ótima: erro de generalizaçaõ minimo\n",
    "    - <img src=\"vies_variancia.png\" width=\"400\"> \n",
    "\n",
    "* **overparametrization**:\n",
    "    * expansão do conceito anterior -> dependencia dos parametros\n",
    "        * proposta inteira EMPÍRICA (sem comprovaçaõ matemática ainda)\n",
    "    * a rede enquanto \"underparametrized\", quanto mais complexidade de funções, mais ela DECORA o dataset, até que chegamos no \"interpolation threshhold\", em que a rede decora tudo que há de dataset e a partir daí aprende a interpolar as respostas com o dataset decorado. \n",
    "        * a rede volta aprender e reduzir seu regime, voltando a ter uma generalização aceitável \n",
    "    * <img src=\"overparametrization.png\" width=\"400\"> \n",
    "\n",
    "- **Hipótese do bilhete de loteria**:\n",
    "    - inicializar aleatoriamente uma rede densa com Θo\n",
    "    - treinar a rede até atingir convergência com parâmetros Θ\n",
    "    - Podar Θ e criar uma máscara m\n",
    "    - A configuração de inicialização “winning ticket” é Θ ⊗ m (produto interno)\n",
    "\n",
    "* **Ataques adversariais**:\n",
    "    * enganar a rede neural com alguns pixels e coódigos de pixels que traduzem features aprendidas de forma \"artifical\", podendo confundir a rede \n",
    "    * <img src=\"NN_fooled.png\" width=\"400\">\n",
    "    * <img src=\"pixel_attack2.png\" width=\"400\">\n",
    "    * note que adicionar um ponto em cada imagem era muito influente na rede, de tal forma que o ponto branco no centro deu para a rede a certeza falsa de que era um avião\n",
    "\n",
    "- Zhang et al (2017): \"... nossos experimentos estabeleceram que redes convolucionais profundas do estado da arte (...) facilmente ajustam rótulos aleatórios nos dados de treinamento.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estratégias para melhorar a generalização\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
