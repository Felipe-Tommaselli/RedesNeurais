{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 8 TEORIA- Redes Recorrentes\n",
    "\n",
    "19/10 - Aula assíncrona\n",
    "> https://www.youtube.com/watch?v=OzC_vwAqRTU&ab_channel=MoacirAntonelliPonti\n",
    "\n",
    "* Redes neurais para dados sequenciais e diferentes tipos de problemas\n",
    "* Camada recorrente\n",
    "* LSTM\n",
    "* GRU\n",
    "* \"Lookback\" no treinamento de redes recorrentes\n",
    "\n",
    "---\n",
    "\n",
    "## Redes Recorrentes\n",
    "\n",
    "- Dados sequenciais (recorrência)\n",
    "- Camada recorrente básica (RNN)\n",
    "- LSTMs e GRUs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados sequenciais (recorrência)\n",
    "\n",
    "* dados recorrentes -> depênn de um contexto anterior (ex: texto, áudio, vídeo, TEMPO...)\n",
    "\n",
    "- redes neurais convencionais não são capazes de lidar com dados sequenciais\n",
    "    - cada entrada é independente da anterior no modelo original\n",
    "    - Camadas densas e convolucionais consideram apenas o exemplo atual para computar a saída\n",
    "    - para isso -> usamos **redes recorrentes**\n",
    "\n",
    "* redes recorrentes tem \"memória\" dos dados passados -> a ordem dos elementos de entrada IMPORTA\n",
    "    * possibilidades de RNNs:\n",
    "        * 1 entrada, saida sequencial\n",
    "            * ex: imagem de entrada e saida, sequencial de palavras descevendo a imagem \"casa\", \"com\", \"sol\", \"azul\".\n",
    "        * entrada sequencial, 1 saida\n",
    "            * ex: texto com opinião e a saida é classificaçaõ de sentimento (positivo ou negativo)\n",
    "        * entrada sequencial, saida sequencial\n",
    "            * ex: tradução de texto de um idioma para outro\n",
    "\n",
    "- *obs:* em tradução de texto é interssante por um atraso, para que acumulem algumas palavras e ele não faça uma tradução literal palavra pro palavra, mas sim por um contexto local \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camada recorrente básica (RNN)\n",
    "\n",
    "* <img src=\"RNN.png\" width=\"300\">\n",
    "\n",
    "- ut = Wh * ht-1 + Wx * xt + b\n",
    "    - ht-1 = saída da camada anterior\n",
    "    - xt = entrada atual\n",
    "    - Wh = pesos da camada recorrente\n",
    "    - Wx = pesos da camada de entrada\n",
    "    - b = bias\n",
    "\n",
    "* ht = f(ut)\n",
    "    - f = função de ativação (ex: tanh, sigmoid, relu, etc)\n",
    "    - ht é chamado de variável de memória ou sumário\n",
    "\n",
    "* y = Wy * ht + by\n",
    "    * ht = saída da camada recorrente\n",
    "    * Wy = pesos da camada de saída\n",
    "    * by = bias da camada de saída\n",
    "\n",
    "- **Exemplo:** Predizer próximo caracter \n",
    "    - definimos uma codificação one-hot para os caracteres\n",
    "        - h = [1,0,0,0]\n",
    "        - e = [0,1,0,0]\n",
    "        - l = [0,0,1,0]\n",
    "        - o = [0,0,0,1]\n",
    "    - rede para predizer:\n",
    "    - <img src=\"caracter.png\" width=\"300\">\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a33b474888067a6169f866e52f630d6f3672d35114c8362b477a93e2a003ce7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
